{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AI-based denoising\n",
    "\n",
    "Denoising based on the deep learning model proposed in [1]_.\n",
    "\n",
    "Applied to the original data from [1]_ and a slice from [2]_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "import ramanspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 19\n",
    "METRICS = ['MSE', 'SAD', 'SID']\n",
    "colors = list(plt.cm.get_cmap()(np.linspace(0, 1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create denoisers\n",
    "We will start by defining the denoisers we will use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-based denoiser\n",
    "First, we will define and load the AI-based denoiser. This is based on the deep learning model proposed in [1]_. The model\n",
    "is pretrained on pairs of low-signal-to-noise (SNR) and high-SNR spectra. The trained model has been deposited by the authors\n",
    "on [GitHub](https://github.com/conor-horgan/DeepeR/blob/master/Raman%20Spectral%20Denoising/model.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"MIT License\n",
    "\n",
    "Copyright (c) 2020 conor-horgan\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, batch_norm):\n",
    "        super(BasicConv, self).__init__()\n",
    "        basic_conv = [nn.Conv1d(channels_in, channels_out, kernel_size=3, stride=1, padding=1, bias=True)]\n",
    "        basic_conv.append(nn.PReLU())\n",
    "        if batch_norm:\n",
    "            basic_conv.append(nn.BatchNorm1d(channels_out))\n",
    "\n",
    "        self.body = nn.Sequential(*basic_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class ResUNetConv(nn.Module):\n",
    "    def __init__(self, num_convs, channels, batch_norm):\n",
    "        super(ResUNetConv, self).__init__()\n",
    "        unet_conv = []\n",
    "        for _ in range(num_convs):\n",
    "            unet_conv.append(nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1, bias=True))\n",
    "            unet_conv.append(nn.PReLU())\n",
    "            if batch_norm:\n",
    "                unet_conv.append(nn.BatchNorm1d(channels))\n",
    "\n",
    "        self.body = nn.Sequential(*unet_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "\n",
    "class UNetLinear(nn.Module):\n",
    "    def __init__(self, repeats, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for i in range(repeats):\n",
    "            modules.append(nn.Linear(channels_in, channels_out))\n",
    "            modules.append(nn.PReLU())\n",
    "\n",
    "        self.body = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, num_convs, batch_norm):\n",
    "        super(ResUNet, self).__init__()\n",
    "        res_conv1 = [BasicConv(1, 64, batch_norm)]\n",
    "        res_conv1.append(ResUNetConv(num_convs, 64, batch_norm))\n",
    "        self.conv1 = nn.Sequential(*res_conv1)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        res_conv2 = [BasicConv(64, 128, batch_norm)]\n",
    "        res_conv2.append(ResUNetConv(num_convs, 128, batch_norm))\n",
    "        self.conv2 = nn.Sequential(*res_conv2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        res_conv3 = [BasicConv(128, 256, batch_norm)]\n",
    "        res_conv3.append(ResUNetConv(num_convs, 256, batch_norm))\n",
    "        res_conv3.append(BasicConv(256, 128, batch_norm))\n",
    "        self.conv3 = nn.Sequential(*res_conv3)\n",
    "        self.up3 = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        res_conv4 = [BasicConv(256, 128, batch_norm)]\n",
    "        res_conv4.append(ResUNetConv(num_convs, 128, batch_norm))\n",
    "        res_conv4.append(BasicConv(128, 64, batch_norm))\n",
    "        self.conv4 = nn.Sequential(*res_conv4)\n",
    "        self.up4 = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        res_conv5 = [BasicConv(128, 64, batch_norm)]\n",
    "        res_conv5.append(ResUNetConv(num_convs, 64, batch_norm))\n",
    "        self.conv5 = nn.Sequential(*res_conv5)\n",
    "        res_conv6 = [BasicConv(64, 1, batch_norm)]\n",
    "        self.conv6 = nn.Sequential(*res_conv6)\n",
    "\n",
    "        self.linear7 = UNetLinear(3, 500, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x1 = self.pool1(x)\n",
    "\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.pool1(x2)\n",
    "\n",
    "        x3 = self.conv3(x3)\n",
    "        x3 = self.up3(x3)\n",
    "\n",
    "        x4 = torch.cat((x2, x3), dim=1)\n",
    "        x4 = self.conv4(x4)\n",
    "        x5 = self.up4(x4)\n",
    "\n",
    "        x6 = torch.cat((x, x5), dim=1)\n",
    "        x6 = self.conv5(x6)\n",
    "        x7 = self.conv6(x6)\n",
    "\n",
    "        out = self.linear7(x7)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = ResUNet(3, False).float()\n",
    "net.load_state_dict(torch.load(r\"ResUNet.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, we define a preprocessing step based on the pretrained model by wrapping it as a PreprocessingStep instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_preprocesing(spectral_data, wavenumber_axis):\n",
    "    flat_spectral_data = spectral_data.reshape(-1, spectral_data.shape[-1])\n",
    "\n",
    "    output = net(torch.Tensor(flat_spectral_data).unsqueeze(1)).cpu().detach().numpy()\n",
    "    output = np.squeeze(output)\n",
    "\n",
    "    output = output.reshape(spectral_data.shape)\n",
    "\n",
    "    return output, wavenumber_axis\n",
    "\n",
    "\n",
    "nn_denoiser = ramanspy.preprocessing.PreprocessingStep(nn_preprocesing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline denoisers\n",
    "\n",
    "Next, we define a set of baseline correction denoisers based on Savitzky-Golay filtering [3]_ with different parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseliners = {\n",
    "    'SG (2, 5)': ramanspy.preprocessing.denoise.SavGol(window_length=5, polyorder=2),\n",
    "    'SG (2, 7)': ramanspy.preprocessing.denoise.SavGol(window_length=7, polyorder=2),\n",
    "    'SG (2, 9)': ramanspy.preprocessing.denoise.SavGol(window_length=9, polyorder=2),\n",
    "    'SG (3, 5)': ramanspy.preprocessing.denoise.SavGol(window_length=5, polyorder=3),\n",
    "    'SG (3, 7)': ramanspy.preprocessing.denoise.SavGol(window_length=7, polyorder=3),\n",
    "    'SG (3, 9)': ramanspy.preprocessing.denoise.SavGol(window_length=9, polyorder=3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods\n",
    "We will also define a set of utility methods to help us extract and compare the results of the different denoisers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a MinMax scalar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minmax = ramanspy.preprocessing.normalise.MinMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method that extracts the results achieved by a denoiser with respect to the defined metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_results(spectrum_to_denoise, target, denoiser):\n",
    "    # Normalise input and output to 0-1\n",
    "    spectrum_to_denoise = minmax.apply(spectrum_to_denoise)\n",
    "    target = minmax.apply(target)\n",
    "\n",
    "    output = denoiser.apply(spectrum_to_denoise)\n",
    "\n",
    "    metrics_result = {metric: getattr(ramanspy.metrics, metric)(output, target) for metric in METRICS}\n",
    "\n",
    "    return output, metrics_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a small utility method to plot the results of comparing the baseline and the AI-based denoiser across the defined metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_results(nn_results_df, baseline_results_dfs):\n",
    "    for metric in METRICS:\n",
    "        plt.figure(figsize=(4, 6), tight_layout=True)\n",
    "\n",
    "        bar_kwargs = {'linewidth': 2, 'zorder': 5}\n",
    "        err_kwargs = {'zorder': 0, 'fmt': 'none', 'linewidth': 2, 'ecolor': 'k', 'capsize': 5}\n",
    "\n",
    "        combined_df = pd.concat([nn_results_df[metric], *[df[metric] for df in baseline_results_dfs.values()]], axis=1,\n",
    "                                ignore_index=True)\n",
    "        combined_df.columns = ['NN'] + list(baseline_results_dfs.keys())\n",
    "\n",
    "        # Plot\n",
    "        means = combined_df.mean()\n",
    "        stds = combined_df.std()\n",
    "        labels = combined_df.columns\n",
    "\n",
    "        sg_cmap = LinearSegmentedColormap.from_list('', [colors[1], [1, 1, 1, 1]])\n",
    "        colors_to_use = list(sg_cmap(np.linspace(0, 1, len(baseliners.keys()) + 2)))[:-2]\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.bar(labels, means, color=[colors[3]] + colors_to_use[::-1], **bar_kwargs)\n",
    "        ax.errorbar(labels, means, yerr=[[0] * len(stds), stds], **err_kwargs)\n",
    "\n",
    "        # Significance tests\n",
    "        combined_df_ = combined_df.melt(var_name='Denoiser', value_name=metric)\n",
    "        box_pairs = [('NN', base) for base in baseliners.keys()]\n",
    "        annotator = Annotator(ax, box_pairs, data=combined_df_, x=\"Denoiser\", y=metric)\n",
    "        annotator.configure(test='Wilcoxon', text_format='star', loc='inside', comparisons_correction='fdr_bh')\n",
    "        annotator.apply_and_annotate()\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data (MDA_MB_231 cells)\n",
    "We will use the MDA_MB_231 cells dataset from the original paper [1]_ to investigate the performance of the proposed\n",
    "AI-based denoiser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "We will use the test set of the MDA_MB_231 cells dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_ = r'../../../data/horgan_data'\n",
    "MDA_MB_231_X_test, MDA_MB_231_Y_test = ramanspy.datasets.MDA_MB_231_cells(dataset='test', folder=dir_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example spectrum\n",
    "Denoising results on an example spectrum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "selected_index = np.random.randint(0, MDA_MB_231_X_test.shape[0])\n",
    "selected_input, selected_target = MDA_MB_231_X_test[selected_index], MDA_MB_231_Y_test[selected_index]\n",
    "\n",
    "nn_results = get_results(selected_input, selected_target, nn_denoiser)[0]\n",
    "baseline_results = get_results(selected_input, selected_target, baseliners['SG (3, 9)'])[0]\n",
    "\n",
    "results = minmax.apply([selected_input, baseline_results, selected_target, nn_results])\n",
    "labels = ['Low SNR Input', 'Savitzky-Golay (3, 9)', 'High SNR Target', 'Neural network']\n",
    "\n",
    "plt.figure(figsize=(10, 4), tight_layout=True)\n",
    "ax = ramanspy.plot.spectra(results, plot_type='single', ylabel='Normalised intensity', title='Original dataset', color=colors)\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on entire dataset\n",
    "Denoising results on the entire testing dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_baseline_results_dfs = {k: pd.DataFrame(columns=METRICS) for k in baseliners.keys()}\n",
    "original_nn_results_df = pd.DataFrame(columns=METRICS)\n",
    "for input, target in zip(MDA_MB_231_X_test, MDA_MB_231_Y_test):\n",
    "    original_nn_results_df = pd.concat([original_nn_results_df, pd.DataFrame([get_results(input, target, nn_denoiser)[1]])], ignore_index=True)\n",
    "\n",
    "    for name, denoiser in baseliners.items():\n",
    "        original_baseline_results_dfs[name] = pd.concat([original_baseline_results_dfs[name], pd.DataFrame([get_results(input, target, denoiser)[1]])], ignore_index=True)\n",
    "\n",
    "show_results(original_nn_results_df, original_baseline_results_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer data (THP-1 cells)\n",
    "To showcase transferability and study generalisation, we will repeat the same experiment on a different unseen dataset.\n",
    "We will use the THP-1 cells dataset from [2]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here, we will load the fifth image layer of the first THP-1 cell sample. Note that we have already preprocessed the data\n",
    "following the same preprocessing steps as in [1]_ for consistency. This included spectral cropping to the 500-1800 cm :sup:`-1` range,\n",
    "followed by baseline correction using the 'shape' method in the WITec Project FIVE software with $\\alpha = 500$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thp_slice = ramanspy.load.witec(r\"3D THP1 map 001 L5 (B+R) (Sub BG).mat\")\n",
    "thp_slice = thp_slice.flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple method to add normal noise to a spectrum that we will use to generate noisy spectra for each spectrum in the\n",
    "THP-1 cells data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_normal_noise(spectrum, std=0.15):\n",
    "    spectrum = ramanspy.preprocessing.normalise.MinMax().apply(spectrum)\n",
    "\n",
    "    # add noise\n",
    "    noise = np.random.normal(0, std, len(spectrum.spectral_data))\n",
    "    noisy_spectrum = ramanspy.Spectrum(spectrum.spectral_data + noise, spectrum.spectral_axis)\n",
    "\n",
    "    return noisy_spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example spectrum\n",
    "Denoising results on an example spectrum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "selected_index = np.random.randint(0, thp_slice.shape[0])\n",
    "selected_target = thp_slice[selected_index]\n",
    "selected_input = add_normal_noise(selected_target)\n",
    "\n",
    "nn_results = get_results(selected_input, selected_target, nn_denoiser)[0]\n",
    "baseline_results = get_results(selected_input, selected_target, baseliners['SG (3, 9)'])[0]\n",
    "\n",
    "results = minmax.apply([selected_input, baseline_results, selected_target, nn_results])\n",
    "labels = ['Input (data with noise)', 'Savitzky-Golay (3, 9)', 'Target (authentic data)', 'Neural network']\n",
    "\n",
    "plt.figure(figsize=(10, 4), tight_layout=True)\n",
    "ax = ramanspy.plot.spectra(results, plot_type='single', ylabel='Normalised intensity', title='Transfer dataset', color=colors)\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on transfer data\n",
    "Denoising results on the entire transfer dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer_baseline_results_dfs = {k: pd.DataFrame(columns=METRICS) for k in baseliners.keys()}\n",
    "transfer_nn_results_df = pd.DataFrame(columns=METRICS)\n",
    "for spectrum in thp_slice:\n",
    "    spectrum_with_noise = add_normal_noise(spectrum)\n",
    "    transfer_nn_results_df = pd.concat([transfer_nn_results_df, pd.DataFrame([get_results(spectrum_with_noise, spectrum, nn_denoiser)[1]])], ignore_index=True)\n",
    "\n",
    "    for name, denoiser in baseliners.items():\n",
    "        transfer_baseline_results_dfs[name] = pd.concat([transfer_baseline_results_dfs[name], pd.DataFrame([get_results(spectrum_with_noise, spectrum, denoiser)[1]])], ignore_index=True)\n",
    "\n",
    "show_results(transfer_nn_results_df, transfer_baseline_results_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    ".. [1] Horgan, C.C., Jensen, M., Nagelkerke, A., St-Pierre, J.P., Vercauteren, T., Stevens, M.M. and Bergholt, M.S., 2021. High-Throughput Molecular Imaging via Deep-Learning-Enabled Raman Spectroscopy. Analytical Chemistry, 93(48), pp.15850-15860.\n",
    "\n",
    ".. [2] Kallepitis, C., Bergholt, M., Mazo, M. et al. Quantitative volumetric Raman imaging of three dimensional cell cultures. Nat Commun 8, 14843 (2017).\n",
    "\n",
    ".. [3] Savitzky A, Golay MJ. Smoothing and differentiation of data by simplified least squares procedures. Analytical chemistry. 1964 Jul 1;36(8):1627-39.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
