
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/vi-analysis/plot_v_integrative_nn.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_vi-analysis_plot_v_integrative_nn.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_vi-analysis_plot_v_integrative_nn.py:


Integrative analysis: Neural Network (NN) classification
=========================================================================

In this example, we will showcase `RamanSPy's` integrability by integrating a Neural Network (NN)
model for the identification of different bacteria species.

To build the model, we will use the `tensorflow <https://tensorflow.org/>`_ Python framework, but similar integrative
analyses are possible with the rest of the Python machine learning and deep learning ecosystem.

The data we will use is the :ref:`Bacteria data`, which is integrated into `RamanSPy`.

.. GENERATED FROM PYTHON SOURCE LINES 14-25

.. code-block:: default



    import tensorflow as tf
    import numpy as np
    from sklearn.metrics import accuracy_score, confusion_matrix
    from sklearn.utils import shuffle
    import seaborn as sns
    import matplotlib.pyplot as plt

    import ramanspy








.. GENERATED FROM PYTHON SOURCE LINES 29-30

First, we will use `RamanSPy` to load the validation and testing bacteria datasets.

.. GENERATED FROM PYTHON SOURCE LINES 30-35

.. code-block:: default

    dir_ = r"../../../../data/bacteria_data"

    X_train, y_train = ramanspy.datasets.bacteria("val", folder=dir_)
    X_test, y_test = ramanspy.datasets.bacteria("test", folder=dir_)








.. GENERATED FROM PYTHON SOURCE LINES 36-37

Shuffling the dataset we will use to train the model.

.. GENERATED FROM PYTHON SOURCE LINES 37-40

.. code-block:: default

    X_train, y_train = shuffle(X_train.flat.spectral_data, y_train)









.. GENERATED FROM PYTHON SOURCE LINES 41-42

Then, we construct the CNN model.

.. GENERATED FROM PYTHON SOURCE LINES 42-54

.. code-block:: default

    class NN(tf.keras.Model):
        def __init__(self, input_dim, output_dim):
            super().__init__()

            self.nn = tf.keras.models.Sequential()
            self.nn.add(tf.keras.Input(shape=(input_dim,)))
            self.nn.add(tf.keras.layers.Dense(output_dim, activation='softmax'))

        def call(self, x):
            return self.nn(x)









.. GENERATED FROM PYTHON SOURCE LINES 55-56

Initialising the model instance

.. GENERATED FROM PYTHON SOURCE LINES 56-67

.. code-block:: default

    learning_rate = 0.001
    batch_size = 32
    epochs = 15
    input_dim = X_train.shape[-1]
    output_dim = len(np.unique(y_train))

    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    model = NN(input_dim, output_dim)
    model.compile(opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
    WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.




.. GENERATED FROM PYTHON SOURCE LINES 68-69

Training the MLP model on the training dataset.

.. GENERATED FROM PYTHON SOURCE LINES 69-72

.. code-block:: default

    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/15
     1/94 [..............................] - ETA: 9s - loss: 3.7386 - accuracy: 0.0000e+00    94/94 [==============================] - 0s 455us/step - loss: 3.2744 - accuracy: 0.1053
    Epoch 2/15
     1/94 [..............................] - ETA: 0s - loss: 3.0171 - accuracy: 0.2812    67/94 [====================>.........] - ETA: 0s - loss: 2.8913 - accuracy: 0.2593    86/94 [==========================>...] - ETA: 0s - loss: 2.8466 - accuracy: 0.2613    94/94 [==============================] - 0s 1ms/step - loss: 2.8323 - accuracy: 0.2623
    Epoch 3/15
     1/94 [..............................] - ETA: 0s - loss: 2.8584 - accuracy: 0.2500    45/94 [=============>................] - ETA: 0s - loss: 2.6218 - accuracy: 0.3285    94/94 [==============================] - 0s 978us/step - loss: 2.5482 - accuracy: 0.3450
    Epoch 4/15
     1/94 [..............................] - ETA: 0s - loss: 2.5386 - accuracy: 0.3125    94/94 [==============================] - 0s 473us/step - loss: 2.3107 - accuracy: 0.4450
    Epoch 5/15
     1/94 [..............................] - ETA: 0s - loss: 2.1514 - accuracy: 0.5625    37/94 [==========>...................] - ETA: 0s - loss: 2.1827 - accuracy: 0.4696    94/94 [==============================] - 0s 906us/step - loss: 2.1440 - accuracy: 0.5080
    Epoch 6/15
     1/94 [..............................] - ETA: 0s - loss: 2.1441 - accuracy: 0.5938    94/94 [==============================] - 0s 409us/step - loss: 1.9855 - accuracy: 0.5690
    Epoch 7/15
     1/94 [..............................] - ETA: 0s - loss: 1.8974 - accuracy: 0.5938    94/94 [==============================] - 0s 424us/step - loss: 1.8557 - accuracy: 0.6260
    Epoch 8/15
     1/94 [..............................] - ETA: 0s - loss: 1.5191 - accuracy: 0.7188    94/94 [==============================] - 0s 414us/step - loss: 1.7328 - accuracy: 0.6693
    Epoch 9/15
     1/94 [..............................] - ETA: 0s - loss: 1.6860 - accuracy: 0.7188    94/94 [==============================] - 0s 429us/step - loss: 1.6225 - accuracy: 0.7320
    Epoch 10/15
     1/94 [..............................] - ETA: 0s - loss: 1.6597 - accuracy: 0.8125    94/94 [==============================] - 0s 448us/step - loss: 1.5356 - accuracy: 0.7543
    Epoch 11/15
     1/94 [..............................] - ETA: 0s - loss: 1.2690 - accuracy: 0.9375    94/94 [==============================] - 0s 432us/step - loss: 1.4531 - accuracy: 0.7780
    Epoch 12/15
     1/94 [..............................] - ETA: 0s - loss: 1.2938 - accuracy: 0.8125    94/94 [==============================] - 0s 395us/step - loss: 1.3831 - accuracy: 0.7823
    Epoch 13/15
     1/94 [..............................] - ETA: 0s - loss: 1.3732 - accuracy: 0.7812    94/94 [==============================] - 0s 386us/step - loss: 1.3060 - accuracy: 0.8160
    Epoch 14/15
     1/94 [..............................] - ETA: 0s - loss: 1.3585 - accuracy: 0.7500    94/94 [==============================] - 0s 395us/step - loss: 1.2404 - accuracy: 0.8373
    Epoch 15/15
     1/94 [..............................] - ETA: 0s - loss: 1.2004 - accuracy: 0.8125    94/94 [==============================] - 0s 385us/step - loss: 1.1869 - accuracy: 0.8253




.. GENERATED FROM PYTHON SOURCE LINES 73-74

Testing the trained model on the unseen testing dataset.

.. GENERATED FROM PYTHON SOURCE LINES 74-80

.. code-block:: default

    y_pred = model.predict(X_test.flat.spectral_data)
    y_pred = np.argmax(y_pred, axis=1)

    print(f"The accuracy of the NN model is: {accuracy_score(y_pred, y_test)}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

     1/94 [..............................] - ETA: 1s    94/94 [==============================] - 0s 251us/step
    The accuracy of the NN model is: 0.6713333333333333




.. GENERATED FROM PYTHON SOURCE LINES 81-82

Confusion matrix:

.. GENERATED FROM PYTHON SOURCE LINES 82-86

.. code-block:: default

    cf_matrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(cf_matrix, annot=True)
    plt.show()




.. image-sg:: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_001.png
   :alt: plot v integrative nn
   :srcset: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 87-88

Accuracy profile:

.. GENERATED FROM PYTHON SOURCE LINES 88-95

.. code-block:: default

    plt.plot(history.history['accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()




.. image-sg:: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_002.png
   :alt: Model accuracy
   :srcset: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 96-97

Loss profile:

.. GENERATED FROM PYTHON SOURCE LINES 97-103

.. code-block:: default

    plt.plot(history.history['loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()



.. image-sg:: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_003.png
   :alt: Model loss
   :srcset: /auto_tutorials/vi-analysis/images/sphx_glr_plot_v_integrative_nn_003.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  8.650 seconds)


.. _sphx_glr_download_auto_tutorials_vi-analysis_plot_v_integrative_nn.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_v_integrative_nn.py <plot_v_integrative_nn.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_v_integrative_nn.ipynb <plot_v_integrative_nn.ipynb>`
